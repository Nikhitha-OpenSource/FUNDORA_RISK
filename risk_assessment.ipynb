{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(\"startupdata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the columns are numeric to avoid type errors\n",
    "cols_to_check = ['Competitive_Density', 'Market_Size_Growth', 'Market_Adoption_Rate', \n",
    "                 'Debt_Equity_Ratio', 'Burn_Rate', 'Founder_Experience', \n",
    "                 'Team_Experience', 'Product_Differentiation']\n",
    "\n",
    "# Convert columns to numeric, coercing errors to NaN\n",
    "data[cols_to_check] = data[cols_to_check].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Handle any NaN values (e.g., fill with 0 or a meaningful placeholder)\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Financial Risk: Higher Debt_Equity_Ratio and Burn_Rate suggest higher financial risk\n",
    "y_financial = (data['Debt_Equity_Ratio'] + data['Burn_Rate']) / 2\n",
    "\n",
    "# Market Risk: Higher Competitive_Density and lower Market_Size_Growth & Market_Adoption_Rate suggest higher market risk\n",
    "y_market = (data['Competitive_Density'] - data['Market_Size_Growth'] - data['Market_Adoption_Rate']) / 3\n",
    "\n",
    "# Operational Risk: Lower experience and education levels with low product differentiation and outdated tech stack increase operational risk\n",
    "y_operational = (1 / (data['Founder_Experience'] + data['Team_Experience'] + 1)) * (1 - data['Product_Differentiation'])\n",
    "\n",
    "# Combine into a target DataFrame\n",
    "y = pd.DataFrame({\n",
    "    'Financial_Risk': y_financial,\n",
    "    'Market_Risk': y_market,\n",
    "    'Operational_Risk': y_operational\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'Location', 'Initial_Funding', 'Funding_Rounds', 'Revenue_Growth_Rate',\n",
    "    'Profit_Margin', 'Annual_Revenue', 'Burn_Rate', 'Market_Size_Growth',\n",
    "    'Competitive_Density', 'Market_Adoption_Rate', 'Founder_Experience',\n",
    "    'Team_Experience', 'Education_Level', 'Product_Differentiation'\n",
    "]\n",
    "X = data[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startup_ID                   0\n",
      "Industry                     0\n",
      "Location                     0\n",
      "Founding_Year                0\n",
      "Initial_Funding              0\n",
      "Funding_Rounds               0\n",
      "Funding_Type                 0\n",
      "Revenue_Growth_Rate          0\n",
      "Profit_Margin                0\n",
      "Annual_Revenue               0\n",
      "Burn_Rate                    0\n",
      "Valuation                    0\n",
      "Debt_Equity_Ratio            0\n",
      "Market_Size_Growth           0\n",
      "Competitive_Density          0\n",
      "Market_Adoption_Rate         0\n",
      "Partnerships                 0\n",
      "Founder_Experience           0\n",
      "Team_Experience              0\n",
      "Education_Level              0\n",
      "Product_Differentiation      0\n",
      "Technology_Stack             0\n",
      "Success_Probability          0\n",
      "Projected_Turnover_Year_1    0\n",
      "Projected_Turnover_Year_2    0\n",
      "Projected_Turnover_Year_3    0\n",
      "Projected_Turnover_Year_4    0\n",
      "Projected_Turnover_Year_5    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Fill missing values with 0 or a meaningful value (e.g., mean/median) for each column\n",
    "data.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the 'Location' column, dropping the first category to avoid multicollinearity\n",
    "X = pd.get_dummies(X, columns=['Location'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "if X.isnull().sum().any() or y.isnull().sum().any():\n",
    "    print(\"There are missing values. Handling them now...\")\n",
    "    X = X.fillna(X.mean())  # Fill missing values with the mean of each column\n",
    "    y = y.fillna(y.mean())  # Fill missing target values with the mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate numerical features from categorical ones\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "X_numerical = X[numerical_features]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling to numerical features only\n",
    "X_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "# Replace the scaled numerical columns in the original dataset\n",
    "X[numerical_features] = X_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Initial_Funding  Funding_Rounds  Revenue_Growth_Rate  Profit_Margin  \\\n",
      "0        -1.621894        0.007056             1.490205       0.353526   \n",
      "1        -0.928268       -1.404139             1.606459      -0.098731   \n",
      "2         2.708215       -0.698541            -1.328942       0.318737   \n",
      "3        -0.272903        0.712653            -1.212689      -1.142401   \n",
      "4         0.736672       -1.404139             0.589241      -0.098731   \n",
      "\n",
      "   Annual_Revenue  Burn_Rate  Market_Size_Growth  Competitive_Density  \\\n",
      "0        0.613396   0.593864            1.378207             1.135965   \n",
      "1       -0.034664   1.487922            1.196497            -0.015870   \n",
      "2       -3.418261  -1.641283           -0.438889             1.135965   \n",
      "3        0.566436  -1.730689           -0.984018            -0.015870   \n",
      "4       -2.037537   0.951487            0.651368            -0.783760   \n",
      "\n",
      "   Market_Adoption_Rate  Founder_Experience  Team_Experience Education_Level  \\\n",
      "0              1.598708                 0.0        -1.137748             PhD   \n",
      "1              1.483742                 0.0        -0.365434        Bachelor   \n",
      "2             -1.447873                 0.0        -1.523906        Bachelor   \n",
      "3              1.138846                 0.0        -1.523906          Master   \n",
      "4             -0.068289                 0.0         0.020724             PhD   \n",
      "\n",
      "   Product_Differentiation  Location_China  Location_Germany  Location_India  \\\n",
      "0                     True           False             False           False   \n",
      "1                     True           False              True           False   \n",
      "2                    False           False             False            True   \n",
      "3                     True           False              True           False   \n",
      "4                     True           False             False           False   \n",
      "\n",
      "   Location_UK  Location_USA  \n",
      "0        False          True  \n",
      "1        False         False  \n",
      "2        False         False  \n",
      "3        False         False  \n",
      "4         True         False  \n"
     ]
    }
   ],
   "source": [
    "# Check the result after preprocessing\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = X.select_dtypes(include=['object']).columns  # Select categorical (non-numerical) columns\n",
    "X = pd.get_dummies(X, columns=categorical_columns, drop_first=True)  # One-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = X.astype(np.float32)  # Convert X to float32\n",
    "y = y.astype(np.float32)  # Convert y to float32 (for multi-output regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\22h51\\Envs\\nikhienv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='sigmoid')  # Three outputs for each risk type\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # Using mean squared error for continuous risk scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array(X)  # Ensure X is a numpy array after scaling\n",
    "\n",
    "# Now proceed with splitting data and training the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and compile the model\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='sigmoid')  # Three outputs for each risk type\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # Using mean squared e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.5750 - mae: 0.5307 - val_loss: 0.3710 - val_mae: 0.3918\n",
      "Epoch 2/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3601 - mae: 0.3832 - val_loss: 0.3553 - val_mae: 0.3693\n",
      "Epoch 3/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3558 - mae: 0.3737 - val_loss: 0.3365 - val_mae: 0.3480\n",
      "Epoch 4/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3319 - mae: 0.3478 - val_loss: 0.3223 - val_mae: 0.3263\n",
      "Epoch 5/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3222 - mae: 0.3311 - val_loss: 0.3201 - val_mae: 0.3194\n",
      "Epoch 6/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3169 - mae: 0.3244 - val_loss: 0.3189 - val_mae: 0.3158\n",
      "Epoch 7/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3197 - mae: 0.3207 - val_loss: 0.3181 - val_mae: 0.3124\n",
      "Epoch 8/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3204 - mae: 0.3155 - val_loss: 0.3179 - val_mae: 0.3103\n",
      "Epoch 9/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3207 - mae: 0.3130 - val_loss: 0.3174 - val_mae: 0.3081\n",
      "Epoch 10/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3328 - mae: 0.3193 - val_loss: 0.3170 - val_mae: 0.3067\n",
      "Epoch 11/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3160 - mae: 0.3050 - val_loss: 0.3173 - val_mae: 0.3064\n",
      "Epoch 12/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3171 - mae: 0.3080 - val_loss: 0.3176 - val_mae: 0.3061\n",
      "Epoch 13/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3137 - mae: 0.3028 - val_loss: 0.3167 - val_mae: 0.3043\n",
      "Epoch 14/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3184 - mae: 0.3060 - val_loss: 0.3166 - val_mae: 0.3036\n",
      "Epoch 15/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3243 - mae: 0.3074 - val_loss: 0.3167 - val_mae: 0.3036\n",
      "Epoch 16/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3165 - mae: 0.3027 - val_loss: 0.3168 - val_mae: 0.3038\n",
      "Epoch 17/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3214 - mae: 0.3061 - val_loss: 0.3167 - val_mae: 0.3030\n",
      "Epoch 18/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3286 - mae: 0.3078 - val_loss: 0.3166 - val_mae: 0.3023\n",
      "Epoch 19/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3171 - mae: 0.3018 - val_loss: 0.3166 - val_mae: 0.3021\n",
      "Epoch 20/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3254 - mae: 0.3039 - val_loss: 0.3172 - val_mae: 0.3028\n",
      "Epoch 21/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3143 - mae: 0.2980 - val_loss: 0.3170 - val_mae: 0.3025\n",
      "Epoch 22/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3196 - mae: 0.3016 - val_loss: 0.3166 - val_mae: 0.3016\n",
      "Epoch 23/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3072 - mae: 0.2912 - val_loss: 0.3167 - val_mae: 0.3017\n",
      "Epoch 24/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3085 - mae: 0.2934 - val_loss: 0.3169 - val_mae: 0.3015\n",
      "Epoch 25/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3181 - mae: 0.2992 - val_loss: 0.3169 - val_mae: 0.3018\n",
      "Epoch 26/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3050 - mae: 0.2916 - val_loss: 0.3165 - val_mae: 0.3007\n",
      "Epoch 27/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3132 - mae: 0.2932 - val_loss: 0.3171 - val_mae: 0.3020\n",
      "Epoch 28/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3140 - mae: 0.2957 - val_loss: 0.3171 - val_mae: 0.3018\n",
      "Epoch 29/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3118 - mae: 0.2939 - val_loss: 0.3172 - val_mae: 0.3015\n",
      "Epoch 30/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3239 - mae: 0.3001 - val_loss: 0.3169 - val_mae: 0.3011\n",
      "Epoch 31/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3114 - mae: 0.2911 - val_loss: 0.3167 - val_mae: 0.3006\n",
      "Epoch 32/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3058 - mae: 0.2871 - val_loss: 0.3175 - val_mae: 0.3019\n",
      "Epoch 33/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3120 - mae: 0.2932 - val_loss: 0.3174 - val_mae: 0.3015\n",
      "Epoch 34/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3212 - mae: 0.2938 - val_loss: 0.3173 - val_mae: 0.3014\n",
      "Epoch 35/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3169 - mae: 0.2939 - val_loss: 0.3182 - val_mae: 0.3028\n",
      "Epoch 36/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3039 - mae: 0.2860 - val_loss: 0.3175 - val_mae: 0.3019\n",
      "Epoch 37/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3099 - mae: 0.2906 - val_loss: 0.3180 - val_mae: 0.3023\n",
      "Epoch 38/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3265 - mae: 0.2960 - val_loss: 0.3188 - val_mae: 0.3031\n",
      "Epoch 39/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3170 - mae: 0.2928 - val_loss: 0.3185 - val_mae: 0.3031\n",
      "Epoch 40/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3120 - mae: 0.2885 - val_loss: 0.3181 - val_mae: 0.3021\n",
      "Epoch 41/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3061 - mae: 0.2832 - val_loss: 0.3186 - val_mae: 0.3029\n",
      "Epoch 42/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3000 - mae: 0.2803 - val_loss: 0.3184 - val_mae: 0.3021\n",
      "Epoch 43/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3208 - mae: 0.2925 - val_loss: 0.3191 - val_mae: 0.3042\n",
      "Epoch 44/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3213 - mae: 0.2938 - val_loss: 0.3184 - val_mae: 0.3024\n",
      "Epoch 45/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3237 - mae: 0.2934 - val_loss: 0.3189 - val_mae: 0.3025\n",
      "Epoch 46/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3071 - mae: 0.2820 - val_loss: 0.3191 - val_mae: 0.3035\n",
      "Epoch 47/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3096 - mae: 0.2860 - val_loss: 0.3200 - val_mae: 0.3044\n",
      "Epoch 48/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3232 - mae: 0.2929 - val_loss: 0.3191 - val_mae: 0.3036\n",
      "Epoch 49/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3235 - mae: 0.2924 - val_loss: 0.3197 - val_mae: 0.3042\n",
      "Epoch 50/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3218 - mae: 0.2906 - val_loss: 0.3192 - val_mae: 0.3030\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3304 - mae: 0.3085  \n",
      "Test Loss: 0.33456364274024963, Test MAE: 0.31424272060394287\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "financial_risk, market_risk, operational_risk = predictions[:, 0], predictions[:, 1], predictions[:, 2]\n",
    "\n",
    "# Convert to risk percentages if needed\n",
    "financial_risk_percent = financial_risk * 100\n",
    "market_risk_percent = market_risk * 100\n",
    "operational_risk_percent = operational_risk * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predicted Financial Risk (%)  Predicted Market Risk (%)  \\\n",
      "0                     62.817703                  99.588776   \n",
      "1                     38.116810                  99.999985   \n",
      "2                     29.406239                  99.999962   \n",
      "3                     43.059654                  99.999931   \n",
      "4                     62.609501                  37.679657   \n",
      "\n",
      "   Predicted Operational Risk (%)  Actual Financial Risk (%)  \\\n",
      "0                        0.584043                  60.500000   \n",
      "1                        1.031462                  30.000002   \n",
      "2                        9.966451                  76.000000   \n",
      "3                       11.894053                  42.500000   \n",
      "4                        2.639676                  58.499996   \n",
      "\n",
      "   Actual Market Risk (%)  Actual Operational Risk (%)  \n",
      "0              124.000000                          0.0  \n",
      "1              217.666672                          0.0  \n",
      "2              227.666656                         10.0  \n",
      "3              227.333328                         12.5  \n",
      "4               49.666668                          0.0  \n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Predicted Financial Risk (%)': financial_risk_percent,\n",
    "    'Predicted Market Risk (%)': market_risk_percent,\n",
    "    'Predicted Operational Risk (%)': operational_risk_percent,\n",
    "    'Actual Financial Risk (%)': y_test['Financial_Risk'].values * 100,\n",
    "    'Actual Market Risk (%)': y_test['Market_Risk'].values * 100,\n",
    "    'Actual Operational Risk (%)': y_test['Operational_Risk'].values * 100\n",
    "})\n",
    "print(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape: (600, 3)\n",
      "predictions shape: (600, 3)\n",
      "y_test type: <class 'pandas.core.frame.DataFrame'>\n",
      "predictions type: <class 'numpy.ndarray'>\n",
      "y_test columns: Index(['Financial_Risk', 'Market_Risk', 'Operational_Risk'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Debugging: Check the structure of y_test and predictions\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "print(\"y_test type:\", type(y_test))\n",
    "print(\"predictions type:\", type(predictions))\n",
    "\n",
    "# If y_test is a DataFrame, inspect the column names\n",
    "if isinstance(y_test, pd.DataFrame):\n",
    "    print(\"y_test columns:\", y_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "   Predicted Financial Risk (%)  Predicted Market Risk (%)  \\\n",
      "0                     62.817703                  99.588776   \n",
      "1                     38.116810                  99.999985   \n",
      "2                     29.406239                  99.999962   \n",
      "3                     43.059654                  99.999931   \n",
      "4                     62.609501                  37.679657   \n",
      "\n",
      "   Predicted Operational Risk (%)  Actual Financial Risk (%)  \\\n",
      "0                        0.584043                  60.500000   \n",
      "1                        1.031462                  30.000002   \n",
      "2                        9.966451                  76.000000   \n",
      "3                       11.894053                  42.500000   \n",
      "4                        2.639676                  58.499996   \n",
      "\n",
      "   Actual Market Risk (%)  Actual Operational Risk (%)  \n",
      "0              124.000000                          0.0  \n",
      "1              217.666672                          0.0  \n",
      "2              227.666656                         10.0  \n",
      "3              227.333328                         12.5  \n",
      "4               49.666668                          0.0  \n",
      "Accuracy for Financial Risk: 0.5367\n",
      "Accuracy for Market Risk: 0.9683\n",
      "Accuracy for Operational Risk: 0.9417\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Step 2: Define a threshold for each risk type (e.g., 0.5 for simplicity)\n",
    "threshold = 0.5\n",
    "\n",
    "# Convert predictions into binary outcomes (high risk = 1, low risk = 0)\n",
    "predicted_risk = (predictions >= threshold).astype(int)\n",
    "\n",
    "# Step 3: Ensure y_test is in the right format (convert it to a NumPy array)\n",
    "y_test = y_test.values  # Convert y_test to NumPy array\n",
    "\n",
    "# Convert y_test to binary outcomes (high risk = 1, low risk = 0)\n",
    "actual_risk = (y_test >= threshold).astype(int)\n",
    "\n",
    "# Step 4: Create a DataFrame with predictions and actual values for comparison\n",
    "financial_risk_percent = predictions[:, 0] * 100  # Convert to percentage\n",
    "market_risk_percent = predictions[:, 1] * 100\n",
    "operational_risk_percent = predictions[:, 2] * 100\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Predicted Financial Risk (%)': financial_risk_percent,\n",
    "    'Predicted Market Risk (%)': market_risk_percent,\n",
    "    'Predicted Operational Risk (%)': operational_risk_percent,\n",
    "    'Actual Financial Risk (%)': y_test[:, 0] * 100,  # y_test is assumed to be in the same shape as predictions\n",
    "    'Actual Market Risk (%)': y_test[:, 1] * 100,\n",
    "    'Actual Operational Risk (%)': y_test[:, 2] * 100\n",
    "})\n",
    "\n",
    "# Step 5: Calculate accuracy for each risk type\n",
    "accuracy_financial = accuracy_score(actual_risk[:, 0], predicted_risk[:, 0])\n",
    "accuracy_market = accuracy_score(actual_risk[:, 1], predicted_risk[:, 1])\n",
    "accuracy_operational = accuracy_score(actual_risk[:, 2], predicted_risk[:, 2])\n",
    "\n",
    "# Step 6: Print the results and accuracy for each risk type\n",
    "print(results.head())  # Preview of the predictions vs actual values\n",
    "print(f\"Accuracy for Financial Risk: {accuracy_financial:.4f}\")\n",
    "print(f\"Accuracy for Market Risk: {accuracy_market:.4f}\")\n",
    "print(f\"Accuracy for Operational Risk: {accuracy_operational:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "model.save('risk-assessment-model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nikhienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
